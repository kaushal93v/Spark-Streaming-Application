{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import geohash as gh\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "def sendDataToDB(iter):\n",
    "    client = MongoClient()\n",
    "    db = client.fit5148_assignment_db\n",
    "    streaming_data = db.StreamingData\n",
    "    sender_1=[]\n",
    "    sender_2, sender_3 = {},{}\n",
    "    for record in iter:\n",
    "            #change record data into json type\n",
    "            data = json.loads(record[1])\n",
    "            #add new attribute as geohash, and do the geohasing for each data\n",
    "            data['geohash']= gh.encode(float(data['latitude']),float(data['longitude']),precision=5)\n",
    "            print(data)\n",
    "            #recoggnize the data from which producer, and save into related sender variable\n",
    "            if(data['sender_id']=='messenger_1'): \n",
    "                data['hotspot'] = \"\"\n",
    "                sender_1.append(data)\n",
    "            elif(data['sender_id']=='messenger_2'):\n",
    "                sender_2= data\n",
    "            else:\n",
    "                sender_3= data\n",
    "    #if sender2 and sender3 all got the data from producer            \n",
    "    if any(sender_2) and any(sender_3):\n",
    "        #if these two at the same location\n",
    "        if(sender_2['geohash']==sender_3['geohash']):\n",
    "            #calculate the average of surface temperature and confidence of two hotspot, and save it into sender2 var \n",
    "            sender_2['surface_temperature_celcius']=\\\n",
    "            int(sender_2['surface_temperature_celcius'])+int(sender_3['surface_temperature_celcius'])/2\n",
    "            sender_2['confidence']=int(sender_2['confidence'])+int(sender_3['confidence'])/2\n",
    "            #set the sender3 as empty\n",
    "            sender_3={}\n",
    "            \n",
    "    #get climate data and compare to hotspot data    \n",
    "    for senderdata in sender_1: \n",
    "        #if only have sender2 data\n",
    "        if any(sender_2):\n",
    "            #check the location is same or not\n",
    "            if(senderdata['geohash']==sender_2['geohash']):\n",
    "                del sender_2['latitude']\n",
    "                del sender_2['longitude']\n",
    "                #if is the same, save the hotspot into climate data\n",
    "                senderdata['hotspot']=sender_2\n",
    "        #if only have sender3 data\n",
    "        if any(sender_3):\n",
    "            #check the location is same or not\n",
    "            if(senderdata['geohash']==sender_3['geohash']):\n",
    "                del sender_3['latitude']\n",
    "                del sender_3['longitude']\n",
    "                #if is the same, save the hotspot into climate data\n",
    "                senderdata['hotspot']=sender_3\n",
    "\n",
    "        try:\n",
    "            streaming_data.insert(senderdata)\n",
    "        except Exception as ex:\n",
    "            print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "    client.close()\n",
    "\n",
    "n_secs = 10\n",
    "topic = \"Climate_Hotpot_Streaming\"\n",
    "\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs)\n",
    "    \n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'group.id':'week12-group', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "                        # Group ID is completely arbitrary\n",
    "\n",
    "lines = kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB))\n",
    "\n",
    "ssc.start()\n",
    "try:\n",
    "    ssc.awaitTermination(timeout=600)\n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    sc.stop()\n",
    "    \n",
    "ssc.stop()\n",
    "sc.stop()\n",
    "    \n",
    "# ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.fit5148_assignment_db\n",
    "streaming_data = db.StreamingData\n",
    "streaming_data.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
